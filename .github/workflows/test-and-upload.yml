# Workflow Name
name: Test Keys and Upload Consolidated

on:
  schedule:
    # Run every 30 minutes (UTC time)
    - cron: '*/30 * * * *'
  workflow_dispatch: # Allows manual triggering from Actions tab

jobs:
  test-and-upload:
    runs-on: ubuntu-latest # Use Linux runner
    timeout-minutes: 20 # Job timeout

    steps:
      # 1. Checkout repository code using PUSH_TOKEN for push access
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PUSH_TOKEN }} # Use PAT for push access later
          persist-credentials: false # Avoid caching token

      # 2. Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Specify Python version
          cache: 'pip' # Cache pip dependencies for faster installs

      # 3. Install Python dependencies (FIXED: Added SOCKS support)
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          # Install requests AND socks support needed for proxy testing
          pip install requests[socks]
          echo "Installed required Python packages (requests with SOCKS)."

      # 4. Run the Python Key Tester Script
      - name: Run Key Tester Script
        id: run-tester
        env:
          # Pass secrets to the Python script environment
          SOURCE_URLS_SECRET: ${{ secrets.SOURCE_URLS_SECRET }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Use the default token for GitHub API calls
        # Execute the Python script
        run: python test_and_upload.py # Ensure this filename matches your script

      # 5. Commit and Push the SINGLE Subscription File using PUSH_TOKEN
      - name: Commit and Push Consolidated Subscription File
        env:
          GIT_PUSH_TOKEN: ${{ secrets.PUSH_TOKEN }} # Pass token explicitly
        run: |
          SUB_DIR="subscription"
          OUTPUT_FILE="working_keys.txt"
          OUTPUT_FILE_PATH="${SUB_DIR}/${OUTPUT_FILE}"

          if [ ! -f "${OUTPUT_FILE_PATH}" ]; then
            echo "Output file '${OUTPUT_FILE_PATH}' not found. Nothing to commit or push."
            echo "changes_pushed=false" >> $GITHUB_OUTPUT
            exit 0 # Exit successfully, nothing to do
          fi

          echo "Configuring Git user..."
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          echo "Adding file '${OUTPUT_FILE_PATH}' to staging..."
          git add ${OUTPUT_FILE_PATH}

          echo "Checking for staged changes..."
          # Use exit code of diff to check for changes
          if git diff --staged --quiet; then
            echo "No changes detected in '${OUTPUT_FILE_PATH}' to commit."
            echo "changes_pushed=false" >> $GITHUB_OUTPUT
          else
            echo "Changes detected. Committing..."
            # Add [skip ci] to prevent triggering another workflow run from this commit
            git commit -m "Update consolidated working keys (${OUTPUT_FILE}) [skip ci]"

            echo "Setting remote URL with token for push..."
            # Use the GITHUB_REPOSITORY variable which is always available
            git remote set-url origin https://x-access-token:${GIT_PUSH_TOKEN}@github.com/${{ github.repository }}

            echo "Pushing changes to branch '${{ github.ref_name }}' (force)..."
            # Use --force with caution. Ensures the repo reflects the latest test results.
            if git push --force origin HEAD:${{ github.ref_name }}; then
               echo "Force push completed successfully."
               echo "changes_pushed=true" >> $GITHUB_OUTPUT
            else
               echo "Error: Force push failed." >&2 # Send error to stderr
               echo "changes_pushed=false" >> $GITHUB_OUTPUT
               exit 1 # Exit with error if push fails (unless continue-on-error is true)
            fi
          fi
        # If the git push fails, we might still want the R2 upload to happen.
        continue-on-error: true # Keep this true to ensure R2 upload attempts even if push fails

      # 6. Install rclone for R2 Upload
      - name: Install rclone
        run: |
          echo "Installing rclone..."
          sudo apt-get update -qq
          sudo apt-get install -y rclone
          echo "rclone installed successfully."
          rclone version # Verify installation

      # 7. Configure rclone for Cloudflare R2 using Secrets
      - name: Configure rclone for Cloudflare R2
        env:
          # Pass R2 credentials safely from repository secrets
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }} # Use account-scoped endpoint secret value
          R2_BUCKET_NAME_DEBUG: ${{ secrets.R2_BUCKET_NAME }} # For debug log only
        run: |
          # Essential check for secrets presence
          echo "--- Verifying R2 Env Vars ---"
          if [ -z "$R2_ACCESS_KEY_ID" ]; then echo "Error: R2_ACCESS_KEY_ID secret is missing or empty." >&2; exit 1; fi
          if [ -z "$R2_SECRET_ACCESS_KEY" ]; then echo "Error: R2_SECRET_ACCESS_KEY secret is missing or empty." >&2; exit 1; fi
          if [ -z "$R2_ENDPOINT" ]; then echo "Error: R2_ENDPOINT secret is missing or empty." >&2; exit 1; fi
          echo "R2_ACCESS_KEY_ID: Set (Length: ${#R2_ACCESS_KEY_ID})"
          echo "R2_ENDPOINT: $R2_ENDPOINT"
          echo "R2_BUCKET_NAME_DEBUG: $R2_BUCKET_NAME_DEBUG"
          echo "--- End R2 Env Var Check ---"

          echo "Creating rclone config for R2 remote named 'R2Storage'..."
          # Configure rclone using environment variables - creates temporary config
          rclone config create R2Storage s3 \
            provider=Cloudflare \
            access_key_id=$R2_ACCESS_KEY_ID \
            secret_access_key=$R2_SECRET_ACCESS_KEY \
            endpoint=$R2_ENDPOINT \
            acl=public-read # Make uploaded file publicly readable
          echo "rclone configured for R2."
          echo "Verifying configuration by listing remotes:"
          rclone listremotes # Should show R2Storage:

      # 8. Upload the SINGLE working key file to R2 Bucket
      - name: Upload working key file to R2 Bucket
        if: success() # Only run if previous steps were successful
        env:
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          LOCAL_FILE="./subscription/working_keys.txt"
          R2_TARGET_PATH="R2Storage:${R2_BUCKET_NAME}/working_keys.txt" # Use configured remote name

          if [ -z "$R2_BUCKET_NAME" ]; then
            echo "Error: R2_BUCKET_NAME secret is missing or empty. Cannot upload." >&2
            exit 1
          fi

          if [ -f "${LOCAL_FILE}" ]; then
            echo "Output file '${LOCAL_FILE}' found. Attempting upload to R2: ${R2_TARGET_PATH}"
            # Removed --verbose. Error messages will still be shown by rclone on failure.
            rclone copyto --checksum --progress --retries 3 --retries-sleep 5s "${LOCAL_FILE}" "${R2_TARGET_PATH}"
            echo "R2 Upload command finished. Check logs above for success/errors."
          else
            echo "Output file '${LOCAL_FILE}' not found. Skipping R2 upload."
          fi

      # 9. Clean up local files (Optional - uncomment if needed)
      # - name: Clean up local files
      #   if: always() # Run even if previous steps fail
      #   run: |
      #     echo "Removing local directory ./subscription/ ..."
      #     rm -rf ./subscription/
      #     echo "Removing xray executable..."
      #     rm -f ./xray
      #     echo "Cleanup complete."
